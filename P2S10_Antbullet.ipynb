{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "P2S10 Antbullet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmulPatil/Reinforcement-learning/blob/master/P2S10_Antbullet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXu1r8qvSzWf",
        "colab_type": "text"
      },
      "source": [
        "# Twin-Delayed DDPG\n",
        "\n",
        "Complete credit goes to this [awesome Deep Reinforcement Learning 2.0 Course on Udemy](https://www.udemy.com/course/deep-reinforcement-learning/) for the code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRzQUhuUTc0J",
        "colab_type": "text"
      },
      "source": [
        "## Installing the packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAHMB0Ze8fU0",
        "colab_type": "code",
        "outputId": "22c87952-d2a5-4f93-a76e-a7efd9bdbd27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "source": [
        "!pip install pybullet"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pybullet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/f5/3d4b55403e40de7892c2f091d95c1db4879204ad8d5fe91e575f850c892e/pybullet-2.7.9.tar.gz (83.7MB)\n",
            "\u001b[K     |████████████████████████████████| 83.7MB 37kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pybullet\n",
            "  Building wheel for pybullet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pybullet: filename=pybullet-2.7.9-cp36-cp36m-linux_x86_64.whl size=95748527 sha256=85013138e31da8574df1bf7e5760dc2bca0b88b23535f1b25dd6c381f3069a27\n",
            "  Stored in directory: /root/.cache/pip/wheels/78/f8/14/4db8932eb7c73725541a93a659efbf9757a935bd47c09d612d\n",
            "Successfully built pybullet\n",
            "Installing collected packages: pybullet\n",
            "Successfully installed pybullet-2.7.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xjm2onHdT-Av",
        "colab_type": "text"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ikr2p0Js8iB4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pybullet_envs\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from gym import wrappers\n",
        "from torch.autograd import Variable\n",
        "from collections import deque"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2nGdtlKVydr",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: We initialize the Experience Replay memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5rW0IDB8nTO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayBuffer(object):\n",
        "\n",
        "  def __init__(self, max_size=1e6):\n",
        "    self.storage = []\n",
        "    self.max_size = max_size\n",
        "    self.ptr = 0\n",
        "\n",
        "  def add(self, transition):\n",
        "    if len(self.storage) == self.max_size:\n",
        "      self.storage[int(self.ptr)] = transition\n",
        "      self.ptr = (self.ptr + 1) % self.max_size\n",
        "    else:\n",
        "      self.storage.append(transition)\n",
        "      self.ptr = (self.ptr + 1) % self.max_size\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
        "    batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [], [], [], [], []\n",
        "    for i in ind: \n",
        "      state, next_state, action, reward, done = self.storage[i]\n",
        "      batch_states.append(np.array(state, copy=False))\n",
        "      batch_next_states.append(np.array(next_state, copy=False))\n",
        "      batch_actions.append(np.array(action, copy=False))\n",
        "      batch_rewards.append(np.array(reward, copy=False))\n",
        "      batch_dones.append(np.array(done, copy=False))\n",
        "    return np.array(batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1), np.array(batch_dones).reshape(-1, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jb7TTaHxWbQD",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: We build one neural network for the Actor model and one neural network for the Actor target"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CeRW4D79HL0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Actor(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    super(Actor, self).__init__()\n",
        "    self.layer_1 = nn.Linear(state_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, action_dim)\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.layer_1(x))\n",
        "    x = F.relu(self.layer_2(x))\n",
        "    x = self.max_action * torch.tanh(self.layer_3(x))\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRDDce8FXef7",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: We build two neural networks for the two Critic models and two neural networks for the two Critic targets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCee7gwR9Jrs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Critic(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    super(Critic, self).__init__()\n",
        "    # Defining the first Critic neural network\n",
        "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, 1)\n",
        "    # Defining the second Critic neural network\n",
        "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_5 = nn.Linear(400, 300)\n",
        "    self.layer_6 = nn.Linear(300, 1)\n",
        "\n",
        "  def forward(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    # Forward-Propagation on the first Critic Neural Network\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    # Forward-Propagation on the second Critic Neural Network\n",
        "    x2 = F.relu(self.layer_4(xu))\n",
        "    x2 = F.relu(self.layer_5(x2))\n",
        "    x2 = self.layer_6(x2)\n",
        "    return x1, x2\n",
        "\n",
        "  def Q1(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    return x1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzIDuONodenW",
        "colab_type": "text"
      },
      "source": [
        "## Steps 4 to 15: Training Process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zzd0H1xukdKe",
        "colab": {}
      },
      "source": [
        "# Selecting the device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Building the whole Training Process into a class\n",
        "\n",
        "class TD3(object):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "    self.critic = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def select_action(self, state):\n",
        "    state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
        "    return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "    \n",
        "    for it in range(iterations):\n",
        "      \n",
        "      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
        "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "      state = torch.Tensor(batch_states).to(device)\n",
        "      next_state = torch.Tensor(batch_next_states).to(device)\n",
        "      action = torch.Tensor(batch_actions).to(device)\n",
        "      reward = torch.Tensor(batch_rewards).to(device)\n",
        "      done = torch.Tensor(batch_dones).to(device)\n",
        "      \n",
        "      # Step 5: From the next state s’, the Actor target plays the next action a’\n",
        "      next_action = self.actor_target(next_state)\n",
        "      \n",
        "      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
        "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
        "      noise = noise.clamp(-noise_clip, noise_clip)\n",
        "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
        "      \n",
        "      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
        "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "      \n",
        "      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
        "      target_Q = torch.min(target_Q1, target_Q2)\n",
        "      \n",
        "      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
        "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
        "      \n",
        "      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
        "      current_Q1, current_Q2 = self.critic(state, action)\n",
        "      \n",
        "      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
        "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "      \n",
        "      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
        "      self.critic_optimizer.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      self.critic_optimizer.step()\n",
        "      \n",
        "      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
        "      if it % policy_freq == 0:\n",
        "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "        \n",
        "        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "        \n",
        "        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "  \n",
        "  # Making a save method to save a trained model\n",
        "  def save(self, filename, directory):\n",
        "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        "  \n",
        "  # Making a load method to load a pre-trained model\n",
        "  def load(self, filename, directory):\n",
        "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ka-ZRtQvjBex",
        "colab_type": "text"
      },
      "source": [
        "## We make a function that evaluates the policy by calculating its average reward over 10 episodes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qabqiYdp9wDM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_policy(policy, eval_episodes=10):\n",
        "  avg_reward = 0.\n",
        "  for _ in range(eval_episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = policy.select_action(np.array(obs))\n",
        "      obs, reward, done, _ = env.step(action)\n",
        "      avg_reward += reward\n",
        "  avg_reward /= eval_episodes\n",
        "  print (\"---------------------------------------\")\n",
        "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
        "  print (\"---------------------------------------\")\n",
        "  return avg_reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGuKmH_ijf7U",
        "colab_type": "text"
      },
      "source": [
        "## We set the parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFj6wbAo97lk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env_name = \"AntBulletEnv-v0\" # Name of a environment (set it to any Continous environment you want)\n",
        "seed = 0 # Random seed number\n",
        "start_timesteps = 1e4 # Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network\n",
        "eval_freq = 5e3 # How often the evaluation step is performed (after how many timesteps)\n",
        "max_timesteps = 5e5 # Total number of iterations/timesteps\n",
        "save_models = True # Boolean checker whether or not to save the pre-trained model\n",
        "expl_noise = 0.1 # Exploration noise - STD value of exploration Gaussian noise\n",
        "batch_size = 100 # Size of the batch\n",
        "discount = 0.99 # Discount factor gamma, used in the calculation of the total discounted reward\n",
        "tau = 0.005 # Target network update rate\n",
        "policy_noise = 0.2 # STD of Gaussian noise added to the actions for the exploration purposes\n",
        "noise_clip = 0.5 # Maximum value of the Gaussian noise added to the actions (policy)\n",
        "policy_freq = 2 # Number of iterations to wait before the policy network (Actor model) is updated"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hjwf2HCol3XP",
        "colab_type": "text"
      },
      "source": [
        "## We create a file name for the two saved models: the Actor and Critic models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fyH8N5z-o3o",
        "colab_type": "code",
        "outputId": "ad7c19cb-9dbd-488d-b11e-762b1da1ab55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Settings: %s\" % (file_name))\n",
        "print (\"---------------------------------------\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Settings: TD3_AntBulletEnv-v0_0\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kop-C96Aml8O",
        "colab_type": "text"
      },
      "source": [
        "## We create a folder inside which will be saved the trained models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Src07lvY-zXb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.exists(\"./results\"):\n",
        "  os.makedirs(\"./results\")\n",
        "if save_models and not os.path.exists(\"./pytorch_models\"):\n",
        "  os.makedirs(\"./pytorch_models\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEAzOd47mv1Z",
        "colab_type": "text"
      },
      "source": [
        "## We create the PyBullet environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyQXJUIs-6BV",
        "colab_type": "code",
        "outputId": "9762f9aa-a725-49ef-eb36-70c57f4c5021",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "env = gym.make(env_name)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YdPG4HXnNsh",
        "colab_type": "text"
      },
      "source": [
        "## We set seeds and we get the necessary information on the states and actions in the chosen environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3RufYec_ADj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWEgDAQxnbem",
        "colab_type": "text"
      },
      "source": [
        "## We create the policy network (the Actor model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTVvG7F8_EWg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "policy = TD3(state_dim, action_dim, max_action)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZI60VN2Unklh",
        "colab_type": "text"
      },
      "source": [
        "[link text](https://)## We create the Experience Replay memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sd-ZsdXR_LgV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "replay_buffer = ReplayBuffer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYOpCyiDnw7s",
        "colab_type": "text"
      },
      "source": [
        "## We define a list where all the evaluation results over 10 episodes are stored"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhC_5XJ__Orp",
        "colab_type": "code",
        "outputId": "e12041d9-8895-4bbb-fd2c-914bf44dc537",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "evaluations = [evaluate_policy(policy)]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 9.804960\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm-4b3p6rglE",
        "colab_type": "text"
      },
      "source": [
        "## We create a new folder directory in which the final results (videos of the agent) will be populated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTL9uMd0ru03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mkdir(base, name):\n",
        "    path = os.path.join(base, name)\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    return path\n",
        "work_dir = mkdir('exp', 'brs')\n",
        "monitor_dir = mkdir(work_dir, 'monitor')\n",
        "max_episode_steps = env._max_episode_steps\n",
        "save_env_vid = False\n",
        "if save_env_vid:\n",
        "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
        "  env.reset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31n5eb03p-Fm",
        "colab_type": "text"
      },
      "source": [
        "## We initialize the variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vN5EvxK_QhT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_timesteps = 0\n",
        "timesteps_since_eval = 0\n",
        "episode_num = 0\n",
        "done = True\n",
        "t0 = time.time()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9gsjvtPqLgT",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_ouY4NH_Y0I",
        "colab_type": "code",
        "outputId": "aa77f90f-f9ce-4d85-d99e-47b7964dd039",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "max_timesteps = 500000\n",
        "# We start the main loop over 500,000 timesteps\n",
        "while total_timesteps < max_timesteps:\n",
        "  \n",
        "  # If the episode is done\n",
        "  if done:\n",
        "\n",
        "    # If we are not at the very beginning, we start the training process of the model\n",
        "    if total_timesteps != 0:\n",
        "      print(\"Total Timesteps: {} Episode Num: {} Reward: {}\".format(total_timesteps, episode_num, episode_reward))\n",
        "      policy.train(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n",
        "\n",
        "    # We evaluate the episode and we save the policy\n",
        "    if timesteps_since_eval >= eval_freq:\n",
        "      timesteps_since_eval %= eval_freq\n",
        "      evaluations.append(evaluate_policy(policy))\n",
        "      policy.save(file_name, directory=\"./pytorch_models\")\n",
        "      np.save(\"./results/%s\" % (file_name), evaluations)\n",
        "    \n",
        "    # When the training step is done, we reset the state of the environment\n",
        "    obs = env.reset()\n",
        "    \n",
        "    # Set the Done to False\n",
        "    done = False\n",
        "    \n",
        "    # Set rewards and episode timesteps to zero\n",
        "    episode_reward = 0\n",
        "    episode_timesteps = 0\n",
        "    episode_num += 1\n",
        "  \n",
        "  # Before 10000 timesteps, we play random actions\n",
        "  if total_timesteps < start_timesteps:\n",
        "    action = env.action_space.sample()\n",
        "  else: # After 10000 timesteps, we switch to the model\n",
        "    action = policy.select_action(np.array(obs))\n",
        "    # If the explore_noise parameter is not 0, we add noise to the action and we clip it\n",
        "    if expl_noise != 0:\n",
        "      action = (action + np.random.normal(0, expl_noise, size=env.action_space.shape[0])).clip(env.action_space.low, env.action_space.high)\n",
        "  \n",
        "  # The agent performs the action in the environment, then reaches the next state and receives the reward\n",
        "  new_obs, reward, done, _ = env.step(action)\n",
        "  \n",
        "  # We check if the episode is done\n",
        "  done_bool = 0 if episode_timesteps + 1 == env._max_episode_steps else float(done)\n",
        "  \n",
        "  # We increase the total reward\n",
        "  episode_reward += reward\n",
        "  \n",
        "  # We store the new transition into the Experience Replay memory (ReplayBuffer)\n",
        "  replay_buffer.add((obs, new_obs, action, reward, done_bool))\n",
        "\n",
        "  # We update the state, the episode timestep, the total timesteps, and the timesteps since the evaluation of the policy\n",
        "  obs = new_obs\n",
        "  episode_timesteps += 1\n",
        "  total_timesteps += 1\n",
        "  timesteps_since_eval += 1\n",
        "\n",
        "# We add the last policy evaluation to our list of evaluations and we save our model\n",
        "evaluations.append(evaluate_policy(policy))\n",
        "if save_models: policy.save(\"%s\" % (file_name), directory=\"./pytorch_models\")\n",
        "np.save(\"./results/%s\" % (file_name), evaluations)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Timesteps: 1000 Episode Num: 1 Reward: 493.27288562482727\n",
            "Total Timesteps: 2000 Episode Num: 2 Reward: 520.7865064411291\n",
            "Total Timesteps: 3000 Episode Num: 3 Reward: 497.5260454541661\n",
            "Total Timesteps: 4000 Episode Num: 4 Reward: 497.3901408542867\n",
            "Total Timesteps: 5000 Episode Num: 5 Reward: 392.8184674976055\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 135.520466\n",
            "---------------------------------------\n",
            "Total Timesteps: 5032 Episode Num: 6 Reward: 11.988981517143841\n",
            "Total Timesteps: 6032 Episode Num: 7 Reward: 491.30788858416224\n",
            "Total Timesteps: 7032 Episode Num: 8 Reward: 514.935098286734\n",
            "Total Timesteps: 8032 Episode Num: 9 Reward: 518.1526959086494\n",
            "Total Timesteps: 8088 Episode Num: 10 Reward: 30.73915695250762\n",
            "Total Timesteps: 8385 Episode Num: 11 Reward: 148.5018022154938\n",
            "Total Timesteps: 9385 Episode Num: 12 Reward: 510.45984040003566\n",
            "Total Timesteps: 9477 Episode Num: 13 Reward: 42.16192708251619\n",
            "Total Timesteps: 10477 Episode Num: 14 Reward: 437.81207676518534\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 140.458355\n",
            "---------------------------------------\n",
            "Total Timesteps: 11477 Episode Num: 15 Reward: 182.82159886603313\n",
            "Total Timesteps: 12477 Episode Num: 16 Reward: 271.4787786231305\n",
            "Total Timesteps: 13477 Episode Num: 17 Reward: 151.33935531277484\n",
            "Total Timesteps: 14477 Episode Num: 18 Reward: 127.57078768775028\n",
            "Total Timesteps: 15477 Episode Num: 19 Reward: 495.3540924751025\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -0.875721\n",
            "---------------------------------------\n",
            "Total Timesteps: 15497 Episode Num: 20 Reward: -0.0017929739555664348\n",
            "Total Timesteps: 15517 Episode Num: 21 Reward: -1.021853765347979\n",
            "Total Timesteps: 15537 Episode Num: 22 Reward: -0.6397137360083658\n",
            "Total Timesteps: 15557 Episode Num: 23 Reward: 0.4853790135928455\n",
            "Total Timesteps: 15577 Episode Num: 24 Reward: 0.9318524810787006\n",
            "Total Timesteps: 15597 Episode Num: 25 Reward: 0.5498568202376655\n",
            "Total Timesteps: 15617 Episode Num: 26 Reward: 0.5281575084807857\n",
            "Total Timesteps: 15637 Episode Num: 27 Reward: 0.2517100774638146\n",
            "Total Timesteps: 15657 Episode Num: 28 Reward: 0.5083830789083583\n",
            "Total Timesteps: 15677 Episode Num: 29 Reward: -0.27804343044081437\n",
            "Total Timesteps: 15697 Episode Num: 30 Reward: -0.3505726195525287\n",
            "Total Timesteps: 16697 Episode Num: 31 Reward: 240.88253047072558\n",
            "Total Timesteps: 17084 Episode Num: 32 Reward: 118.71111761609902\n",
            "Total Timesteps: 18084 Episode Num: 33 Reward: 244.4212417827149\n",
            "Total Timesteps: 19084 Episode Num: 34 Reward: 503.09718345003154\n",
            "Total Timesteps: 19647 Episode Num: 35 Reward: 263.208750379397\n",
            "Total Timesteps: 20647 Episode Num: 36 Reward: 117.70019110146822\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 22.935739\n",
            "---------------------------------------\n",
            "Total Timesteps: 20667 Episode Num: 37 Reward: -0.7518930073276229\n",
            "Total Timesteps: 20687 Episode Num: 38 Reward: -0.22158760601169325\n",
            "Total Timesteps: 20707 Episode Num: 39 Reward: -0.3102351119261897\n",
            "Total Timesteps: 20728 Episode Num: 40 Reward: 0.17639380260652038\n",
            "Total Timesteps: 21514 Episode Num: 41 Reward: 346.95638644062825\n",
            "Total Timesteps: 22514 Episode Num: 42 Reward: 401.5465850584684\n",
            "Total Timesteps: 23514 Episode Num: 43 Reward: 190.32405967194757\n",
            "Total Timesteps: 24514 Episode Num: 44 Reward: 178.01144194255411\n",
            "Total Timesteps: 25514 Episode Num: 45 Reward: 391.49852577846667\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 503.497035\n",
            "---------------------------------------\n",
            "Total Timesteps: 26514 Episode Num: 46 Reward: 358.20280556960574\n",
            "Total Timesteps: 27514 Episode Num: 47 Reward: 600.1987211354844\n",
            "Total Timesteps: 28514 Episode Num: 48 Reward: 452.2906489972451\n",
            "Total Timesteps: 29514 Episode Num: 49 Reward: 343.3762125034449\n",
            "Total Timesteps: 30514 Episode Num: 50 Reward: 389.54079859924417\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 318.000128\n",
            "---------------------------------------\n",
            "Total Timesteps: 31514 Episode Num: 51 Reward: 234.98124062914206\n",
            "Total Timesteps: 32514 Episode Num: 52 Reward: 215.17524632786515\n",
            "Total Timesteps: 33514 Episode Num: 53 Reward: 210.51127217254884\n",
            "Total Timesteps: 34514 Episode Num: 54 Reward: 461.5590456487396\n",
            "Total Timesteps: 35514 Episode Num: 55 Reward: 262.5642960111577\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 596.422722\n",
            "---------------------------------------\n",
            "Total Timesteps: 36514 Episode Num: 56 Reward: 663.9527287458151\n",
            "Total Timesteps: 37514 Episode Num: 57 Reward: 479.784494402137\n",
            "Total Timesteps: 38514 Episode Num: 58 Reward: 458.3834153973677\n",
            "Total Timesteps: 39514 Episode Num: 59 Reward: 352.18694642585444\n",
            "Total Timesteps: 40514 Episode Num: 60 Reward: 346.9294869555957\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 369.719058\n",
            "---------------------------------------\n",
            "Total Timesteps: 41514 Episode Num: 61 Reward: 371.72474281228295\n",
            "Total Timesteps: 42514 Episode Num: 62 Reward: 315.0810698338982\n",
            "Total Timesteps: 43514 Episode Num: 63 Reward: 441.5765675020172\n",
            "Total Timesteps: 44514 Episode Num: 64 Reward: 184.78963088040842\n",
            "Total Timesteps: 45514 Episode Num: 65 Reward: 343.1568562226337\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 434.712703\n",
            "---------------------------------------\n",
            "Total Timesteps: 46514 Episode Num: 66 Reward: 438.2134935134476\n",
            "Total Timesteps: 47514 Episode Num: 67 Reward: 520.1868822826343\n",
            "Total Timesteps: 48514 Episode Num: 68 Reward: 322.621867029994\n",
            "Total Timesteps: 49514 Episode Num: 69 Reward: 184.77953799014372\n",
            "Total Timesteps: 50514 Episode Num: 70 Reward: 295.2182484895426\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 397.673986\n",
            "---------------------------------------\n",
            "Total Timesteps: 51514 Episode Num: 71 Reward: 298.2380347433935\n",
            "Total Timesteps: 52514 Episode Num: 72 Reward: 149.27917290817905\n",
            "Total Timesteps: 53514 Episode Num: 73 Reward: 292.24828840171347\n",
            "Total Timesteps: 54514 Episode Num: 74 Reward: 312.713520455209\n",
            "Total Timesteps: 55514 Episode Num: 75 Reward: 300.0137945061813\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 479.239866\n",
            "---------------------------------------\n",
            "Total Timesteps: 56514 Episode Num: 76 Reward: 592.9905829485558\n",
            "Total Timesteps: 57514 Episode Num: 77 Reward: 509.49804325255957\n",
            "Total Timesteps: 58514 Episode Num: 78 Reward: 662.5817857147653\n",
            "Total Timesteps: 59514 Episode Num: 79 Reward: 279.4070164690223\n",
            "Total Timesteps: 60514 Episode Num: 80 Reward: 193.53691581263448\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 328.668435\n",
            "---------------------------------------\n",
            "Total Timesteps: 61514 Episode Num: 81 Reward: 394.80019990249076\n",
            "Total Timesteps: 62514 Episode Num: 82 Reward: 202.99453644093808\n",
            "Total Timesteps: 63514 Episode Num: 83 Reward: 323.67120281142155\n",
            "Total Timesteps: 64514 Episode Num: 84 Reward: 265.5218821940076\n",
            "Total Timesteps: 65514 Episode Num: 85 Reward: 444.8473186785108\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 358.306471\n",
            "---------------------------------------\n",
            "Total Timesteps: 66514 Episode Num: 86 Reward: 322.69998524544303\n",
            "Total Timesteps: 67514 Episode Num: 87 Reward: 277.85427178088054\n",
            "Total Timesteps: 67537 Episode Num: 88 Reward: 3.235526052756299\n",
            "Total Timesteps: 67929 Episode Num: 89 Reward: 147.17188798663327\n",
            "Total Timesteps: 67961 Episode Num: 90 Reward: 9.362350447302772\n",
            "Total Timesteps: 67984 Episode Num: 91 Reward: 4.076730250955315\n",
            "Total Timesteps: 68984 Episode Num: 92 Reward: 312.68322951559463\n",
            "Total Timesteps: 69021 Episode Num: 93 Reward: 5.175724488180673\n",
            "Total Timesteps: 70021 Episode Num: 94 Reward: 507.15946613874223\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 512.872931\n",
            "---------------------------------------\n",
            "Total Timesteps: 71021 Episode Num: 95 Reward: 673.6274612930426\n",
            "Total Timesteps: 71118 Episode Num: 96 Reward: 48.64389641139026\n",
            "Total Timesteps: 71310 Episode Num: 97 Reward: 89.75788930396084\n",
            "Total Timesteps: 71762 Episode Num: 98 Reward: 222.01375098527325\n",
            "Total Timesteps: 72762 Episode Num: 99 Reward: 509.81434221111857\n",
            "Total Timesteps: 73762 Episode Num: 100 Reward: 537.461186905873\n",
            "Total Timesteps: 74762 Episode Num: 101 Reward: 317.69332278782457\n",
            "Total Timesteps: 75762 Episode Num: 102 Reward: 406.50544606622594\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 486.345671\n",
            "---------------------------------------\n",
            "Total Timesteps: 76762 Episode Num: 103 Reward: 466.1170266252622\n",
            "Total Timesteps: 77762 Episode Num: 104 Reward: 475.15126181231824\n",
            "Total Timesteps: 78762 Episode Num: 105 Reward: 355.6762018729316\n",
            "Total Timesteps: 79762 Episode Num: 106 Reward: 321.54577766073663\n",
            "Total Timesteps: 80762 Episode Num: 107 Reward: 154.6658564425359\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 461.505935\n",
            "---------------------------------------\n",
            "Total Timesteps: 81762 Episode Num: 108 Reward: 448.6377592456029\n",
            "Total Timesteps: 81988 Episode Num: 109 Reward: 61.606507653479106\n",
            "Total Timesteps: 82988 Episode Num: 110 Reward: 293.3756987732954\n",
            "Total Timesteps: 83988 Episode Num: 111 Reward: 341.43889073788046\n",
            "Total Timesteps: 84988 Episode Num: 112 Reward: 340.92746014356976\n",
            "Total Timesteps: 85122 Episode Num: 113 Reward: 64.0940557578415\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 134.086901\n",
            "---------------------------------------\n",
            "Total Timesteps: 86122 Episode Num: 114 Reward: 100.27930012615538\n",
            "Total Timesteps: 87122 Episode Num: 115 Reward: 398.9467588142035\n",
            "Total Timesteps: 88122 Episode Num: 116 Reward: 157.92186594452144\n",
            "Total Timesteps: 89122 Episode Num: 117 Reward: 540.7137921968631\n",
            "Total Timesteps: 90122 Episode Num: 118 Reward: 548.9119388255124\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 491.585022\n",
            "---------------------------------------\n",
            "Total Timesteps: 91122 Episode Num: 119 Reward: 540.8641063911932\n",
            "Total Timesteps: 92122 Episode Num: 120 Reward: 663.1969034915735\n",
            "Total Timesteps: 93122 Episode Num: 121 Reward: 431.3144573830963\n",
            "Total Timesteps: 94122 Episode Num: 122 Reward: 563.0870752613636\n",
            "Total Timesteps: 95122 Episode Num: 123 Reward: 373.83207482919227\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 452.807168\n",
            "---------------------------------------\n",
            "Total Timesteps: 96122 Episode Num: 124 Reward: 614.736626668148\n",
            "Total Timesteps: 97122 Episode Num: 125 Reward: 515.5709598918764\n",
            "Total Timesteps: 98122 Episode Num: 126 Reward: 429.71389114200576\n",
            "Total Timesteps: 99122 Episode Num: 127 Reward: 415.96981095836446\n",
            "Total Timesteps: 100122 Episode Num: 128 Reward: 240.95197575756617\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 487.800857\n",
            "---------------------------------------\n",
            "Total Timesteps: 101122 Episode Num: 129 Reward: 401.2332222406851\n",
            "Total Timesteps: 102122 Episode Num: 130 Reward: 509.3101685659598\n",
            "Total Timesteps: 103122 Episode Num: 131 Reward: 326.54393187268647\n",
            "Total Timesteps: 104122 Episode Num: 132 Reward: 399.28678314276823\n",
            "Total Timesteps: 105122 Episode Num: 133 Reward: 318.44104888271255\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 299.701773\n",
            "---------------------------------------\n",
            "Total Timesteps: 106122 Episode Num: 134 Reward: 648.4154331955206\n",
            "Total Timesteps: 107122 Episode Num: 135 Reward: 274.0236781470136\n",
            "Total Timesteps: 108122 Episode Num: 136 Reward: 563.8839898002441\n",
            "Total Timesteps: 108270 Episode Num: 137 Reward: 50.23986131949353\n",
            "Total Timesteps: 109270 Episode Num: 138 Reward: 370.4392846093953\n",
            "Total Timesteps: 110270 Episode Num: 139 Reward: 309.91205100346224\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 457.966906\n",
            "---------------------------------------\n",
            "Total Timesteps: 111270 Episode Num: 140 Reward: 393.8924685230159\n",
            "Total Timesteps: 111291 Episode Num: 141 Reward: 2.5761066914619\n",
            "Total Timesteps: 111317 Episode Num: 142 Reward: 7.194708646898197\n",
            "Total Timesteps: 112317 Episode Num: 143 Reward: 437.2227144263697\n",
            "Total Timesteps: 113140 Episode Num: 144 Reward: 454.2219303523419\n",
            "Total Timesteps: 114140 Episode Num: 145 Reward: 432.9016477374433\n",
            "Total Timesteps: 115140 Episode Num: 146 Reward: 444.22081491682684\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 482.682094\n",
            "---------------------------------------\n",
            "Total Timesteps: 116140 Episode Num: 147 Reward: 581.408127699405\n",
            "Total Timesteps: 117140 Episode Num: 148 Reward: 477.7573200840123\n",
            "Total Timesteps: 117259 Episode Num: 149 Reward: 19.78364934760474\n",
            "Total Timesteps: 118259 Episode Num: 150 Reward: 554.1515206933877\n",
            "Total Timesteps: 119259 Episode Num: 151 Reward: 517.8400544198272\n",
            "Total Timesteps: 120259 Episode Num: 152 Reward: 481.8424612797511\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 502.057661\n",
            "---------------------------------------\n",
            "Total Timesteps: 121259 Episode Num: 153 Reward: 635.4206062723264\n",
            "Total Timesteps: 122259 Episode Num: 154 Reward: 406.535679665564\n",
            "Total Timesteps: 123259 Episode Num: 155 Reward: 511.5871902494803\n",
            "Total Timesteps: 124259 Episode Num: 156 Reward: 454.4179018142517\n",
            "Total Timesteps: 125259 Episode Num: 157 Reward: 562.9517578149223\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 409.992433\n",
            "---------------------------------------\n",
            "Total Timesteps: 126259 Episode Num: 158 Reward: 322.1171217497839\n",
            "Total Timesteps: 127259 Episode Num: 159 Reward: 514.3488387122159\n",
            "Total Timesteps: 128259 Episode Num: 160 Reward: 427.12723517382386\n",
            "Total Timesteps: 129259 Episode Num: 161 Reward: 314.0326524739719\n",
            "Total Timesteps: 130259 Episode Num: 162 Reward: 476.4103634365796\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 535.905586\n",
            "---------------------------------------\n",
            "Total Timesteps: 131259 Episode Num: 163 Reward: 624.9600558568209\n",
            "Total Timesteps: 132259 Episode Num: 164 Reward: 546.9103842818737\n",
            "Total Timesteps: 133259 Episode Num: 165 Reward: 441.16638488010926\n",
            "Total Timesteps: 134259 Episode Num: 166 Reward: 482.63513908806254\n",
            "Total Timesteps: 135259 Episode Num: 167 Reward: 524.8598162594468\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 538.381705\n",
            "---------------------------------------\n",
            "Total Timesteps: 136259 Episode Num: 168 Reward: 563.1478394490075\n",
            "Total Timesteps: 137259 Episode Num: 169 Reward: 577.5944621148609\n",
            "Total Timesteps: 138259 Episode Num: 170 Reward: 625.5659083058689\n",
            "Total Timesteps: 139259 Episode Num: 171 Reward: 485.2641452381665\n",
            "Total Timesteps: 140259 Episode Num: 172 Reward: 486.1225835144369\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 514.402316\n",
            "---------------------------------------\n",
            "Total Timesteps: 141259 Episode Num: 173 Reward: 514.789000274599\n",
            "Total Timesteps: 142259 Episode Num: 174 Reward: 550.2970172197282\n",
            "Total Timesteps: 143259 Episode Num: 175 Reward: 442.30456013674143\n",
            "Total Timesteps: 144259 Episode Num: 176 Reward: 462.66261433599976\n",
            "Total Timesteps: 145259 Episode Num: 177 Reward: 346.63268887001607\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 520.483945\n",
            "---------------------------------------\n",
            "Total Timesteps: 146259 Episode Num: 178 Reward: 586.7774990129869\n",
            "Total Timesteps: 147259 Episode Num: 179 Reward: 585.0519093540927\n",
            "Total Timesteps: 148259 Episode Num: 180 Reward: 452.99680842448646\n",
            "Total Timesteps: 149259 Episode Num: 181 Reward: 441.0474748000086\n",
            "Total Timesteps: 150259 Episode Num: 182 Reward: 470.4119854508325\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 550.554597\n",
            "---------------------------------------\n",
            "Total Timesteps: 151259 Episode Num: 183 Reward: 534.0409632133828\n",
            "Total Timesteps: 152259 Episode Num: 184 Reward: 426.7584751544383\n",
            "Total Timesteps: 153259 Episode Num: 185 Reward: 384.8539639900562\n",
            "Total Timesteps: 154259 Episode Num: 186 Reward: 429.0838510865752\n",
            "Total Timesteps: 155259 Episode Num: 187 Reward: 501.88499993715317\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 545.372871\n",
            "---------------------------------------\n",
            "Total Timesteps: 156259 Episode Num: 188 Reward: 361.090267507728\n",
            "Total Timesteps: 157259 Episode Num: 189 Reward: 549.9007384636083\n",
            "Total Timesteps: 158259 Episode Num: 190 Reward: 327.3784291843598\n",
            "Total Timesteps: 159259 Episode Num: 191 Reward: 474.31372768692586\n",
            "Total Timesteps: 160259 Episode Num: 192 Reward: 657.8844641890709\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 490.765632\n",
            "---------------------------------------\n",
            "Total Timesteps: 161259 Episode Num: 193 Reward: 667.7946384839269\n",
            "Total Timesteps: 162259 Episode Num: 194 Reward: 484.59377042890736\n",
            "Total Timesteps: 163259 Episode Num: 195 Reward: 199.9563972152601\n",
            "Total Timesteps: 164259 Episode Num: 196 Reward: 606.0997631362355\n",
            "Total Timesteps: 165259 Episode Num: 197 Reward: 630.403962098012\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 534.834914\n",
            "---------------------------------------\n",
            "Total Timesteps: 166259 Episode Num: 198 Reward: 394.20833339893346\n",
            "Total Timesteps: 167259 Episode Num: 199 Reward: 393.6763900996388\n",
            "Total Timesteps: 168259 Episode Num: 200 Reward: 719.6108353386667\n",
            "Total Timesteps: 169259 Episode Num: 201 Reward: 519.8631428849635\n",
            "Total Timesteps: 170259 Episode Num: 202 Reward: 486.54308255873366\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 537.472918\n",
            "---------------------------------------\n",
            "Total Timesteps: 171259 Episode Num: 203 Reward: 545.7284161092335\n",
            "Total Timesteps: 172259 Episode Num: 204 Reward: 624.4619080665188\n",
            "Total Timesteps: 173259 Episode Num: 205 Reward: 551.0112736524011\n",
            "Total Timesteps: 174259 Episode Num: 206 Reward: 642.5287829363743\n",
            "Total Timesteps: 175259 Episode Num: 207 Reward: 534.343200748594\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 418.324593\n",
            "---------------------------------------\n",
            "Total Timesteps: 176259 Episode Num: 208 Reward: 360.19569702627825\n",
            "Total Timesteps: 177259 Episode Num: 209 Reward: 537.5853680751666\n",
            "Total Timesteps: 178259 Episode Num: 210 Reward: 662.4866448480344\n",
            "Total Timesteps: 179259 Episode Num: 211 Reward: 557.1340818307383\n",
            "Total Timesteps: 180259 Episode Num: 212 Reward: 247.8755869873721\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 406.589276\n",
            "---------------------------------------\n",
            "Total Timesteps: 181259 Episode Num: 213 Reward: 581.47971585097\n",
            "Total Timesteps: 182259 Episode Num: 214 Reward: 653.9829407672702\n",
            "Total Timesteps: 183259 Episode Num: 215 Reward: 407.49361274204404\n",
            "Total Timesteps: 184259 Episode Num: 216 Reward: 687.2957031831506\n",
            "Total Timesteps: 185259 Episode Num: 217 Reward: 517.4105505175414\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 526.279520\n",
            "---------------------------------------\n",
            "Total Timesteps: 186259 Episode Num: 218 Reward: 317.2833399784702\n",
            "Total Timesteps: 187259 Episode Num: 219 Reward: 452.2668548653047\n",
            "Total Timesteps: 187424 Episode Num: 220 Reward: 5.630012945364209\n",
            "Total Timesteps: 188424 Episode Num: 221 Reward: 507.7555074215138\n",
            "Total Timesteps: 189424 Episode Num: 222 Reward: 751.1704872186386\n",
            "Total Timesteps: 190424 Episode Num: 223 Reward: 568.3368801210381\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 623.756873\n",
            "---------------------------------------\n",
            "Total Timesteps: 191424 Episode Num: 224 Reward: 571.8476583350208\n",
            "Total Timesteps: 192424 Episode Num: 225 Reward: 360.32788546098897\n",
            "Total Timesteps: 193424 Episode Num: 226 Reward: 514.3855083901035\n",
            "Total Timesteps: 194424 Episode Num: 227 Reward: 575.9432449562902\n",
            "Total Timesteps: 195424 Episode Num: 228 Reward: 583.1366992443653\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 506.711023\n",
            "---------------------------------------\n",
            "Total Timesteps: 196424 Episode Num: 229 Reward: 500.09880738023344\n",
            "Total Timesteps: 197424 Episode Num: 230 Reward: 743.6684401591649\n",
            "Total Timesteps: 198424 Episode Num: 231 Reward: 577.8463058683919\n",
            "Total Timesteps: 199424 Episode Num: 232 Reward: 589.7052766690998\n",
            "Total Timesteps: 200424 Episode Num: 233 Reward: 700.2308720941998\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 545.480484\n",
            "---------------------------------------\n",
            "Total Timesteps: 201424 Episode Num: 234 Reward: 566.8755231864044\n",
            "Total Timesteps: 202424 Episode Num: 235 Reward: 473.4307552389623\n",
            "Total Timesteps: 203424 Episode Num: 236 Reward: 386.83944655495037\n",
            "Total Timesteps: 204424 Episode Num: 237 Reward: 633.9189851854371\n",
            "Total Timesteps: 205424 Episode Num: 238 Reward: 477.91770333299377\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 548.300081\n",
            "---------------------------------------\n",
            "Total Timesteps: 206424 Episode Num: 239 Reward: 481.3542862355146\n",
            "Total Timesteps: 207424 Episode Num: 240 Reward: 662.2089269961893\n",
            "Total Timesteps: 208424 Episode Num: 241 Reward: 677.1176597330257\n",
            "Total Timesteps: 209424 Episode Num: 242 Reward: 400.41388570545365\n",
            "Total Timesteps: 210424 Episode Num: 243 Reward: 575.5785087120856\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 621.584697\n",
            "---------------------------------------\n",
            "Total Timesteps: 211424 Episode Num: 244 Reward: 682.3321645558304\n",
            "Total Timesteps: 212424 Episode Num: 245 Reward: 582.8659887469885\n",
            "Total Timesteps: 213424 Episode Num: 246 Reward: 627.8708564844603\n",
            "Total Timesteps: 214424 Episode Num: 247 Reward: 633.1130831634251\n",
            "Total Timesteps: 215424 Episode Num: 248 Reward: 537.2294313075046\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 567.917398\n",
            "---------------------------------------\n",
            "Total Timesteps: 216424 Episode Num: 249 Reward: 694.9008312469821\n",
            "Total Timesteps: 217424 Episode Num: 250 Reward: 418.1430487489441\n",
            "Total Timesteps: 218424 Episode Num: 251 Reward: 685.5513010541588\n",
            "Total Timesteps: 219424 Episode Num: 252 Reward: 664.2525068123225\n",
            "Total Timesteps: 220424 Episode Num: 253 Reward: 574.9337857508896\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 598.055535\n",
            "---------------------------------------\n",
            "Total Timesteps: 221424 Episode Num: 254 Reward: 558.0660250438024\n",
            "Total Timesteps: 222424 Episode Num: 255 Reward: 589.842026837048\n",
            "Total Timesteps: 223424 Episode Num: 256 Reward: 457.51180298291814\n",
            "Total Timesteps: 224424 Episode Num: 257 Reward: 430.44960014119806\n",
            "Total Timesteps: 225424 Episode Num: 258 Reward: 554.1393177005648\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 631.391163\n",
            "---------------------------------------\n",
            "Total Timesteps: 226424 Episode Num: 259 Reward: 847.5398811469543\n",
            "Total Timesteps: 227424 Episode Num: 260 Reward: 637.4794757426203\n",
            "Total Timesteps: 228424 Episode Num: 261 Reward: 621.4400278262381\n",
            "Total Timesteps: 229424 Episode Num: 262 Reward: 578.2287135108053\n",
            "Total Timesteps: 230424 Episode Num: 263 Reward: 803.9879609314315\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 483.955786\n",
            "---------------------------------------\n",
            "Total Timesteps: 231424 Episode Num: 264 Reward: 384.4991942003888\n",
            "Total Timesteps: 232424 Episode Num: 265 Reward: 734.5018590081899\n",
            "Total Timesteps: 233424 Episode Num: 266 Reward: 688.8602887199527\n",
            "Total Timesteps: 234424 Episode Num: 267 Reward: 746.8785736073439\n",
            "Total Timesteps: 235424 Episode Num: 268 Reward: 789.6387428584152\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 544.380841\n",
            "---------------------------------------\n",
            "Total Timesteps: 236424 Episode Num: 269 Reward: 570.6495176358341\n",
            "Total Timesteps: 237424 Episode Num: 270 Reward: 304.61749874925715\n",
            "Total Timesteps: 238424 Episode Num: 271 Reward: 509.3633927806299\n",
            "Total Timesteps: 239424 Episode Num: 272 Reward: 564.0327916194814\n",
            "Total Timesteps: 240424 Episode Num: 273 Reward: 441.1050542019018\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 692.724698\n",
            "---------------------------------------\n",
            "Total Timesteps: 241424 Episode Num: 274 Reward: 788.1043737159887\n",
            "Total Timesteps: 242424 Episode Num: 275 Reward: 481.23432133730955\n",
            "Total Timesteps: 243424 Episode Num: 276 Reward: 461.64625621406407\n",
            "Total Timesteps: 244424 Episode Num: 277 Reward: 859.0933690129468\n",
            "Total Timesteps: 245424 Episode Num: 278 Reward: 626.9760801150877\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 621.028073\n",
            "---------------------------------------\n",
            "Total Timesteps: 246424 Episode Num: 279 Reward: 490.1497602980206\n",
            "Total Timesteps: 247424 Episode Num: 280 Reward: 653.8316143442906\n",
            "Total Timesteps: 248424 Episode Num: 281 Reward: 651.5605192192609\n",
            "Total Timesteps: 249424 Episode Num: 282 Reward: 794.8289223029294\n",
            "Total Timesteps: 250424 Episode Num: 283 Reward: 790.4184384093811\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 679.179725\n",
            "---------------------------------------\n",
            "Total Timesteps: 251424 Episode Num: 284 Reward: 587.5356957523056\n",
            "Total Timesteps: 252424 Episode Num: 285 Reward: 284.9578129189075\n",
            "Total Timesteps: 253424 Episode Num: 286 Reward: 772.9033294626893\n",
            "Total Timesteps: 254424 Episode Num: 287 Reward: 723.3439173428059\n",
            "Total Timesteps: 255424 Episode Num: 288 Reward: 816.0971030953895\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 794.632485\n",
            "---------------------------------------\n",
            "Total Timesteps: 256424 Episode Num: 289 Reward: 632.9845295049992\n",
            "Total Timesteps: 257424 Episode Num: 290 Reward: 643.7500721869928\n",
            "Total Timesteps: 258424 Episode Num: 291 Reward: 645.7953825421461\n",
            "Total Timesteps: 259424 Episode Num: 292 Reward: 804.8848412403835\n",
            "Total Timesteps: 260424 Episode Num: 293 Reward: 801.5512497775248\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 707.931508\n",
            "---------------------------------------\n",
            "Total Timesteps: 261424 Episode Num: 294 Reward: 727.2463292088604\n",
            "Total Timesteps: 262424 Episode Num: 295 Reward: 806.5059185891191\n",
            "Total Timesteps: 263424 Episode Num: 296 Reward: 760.0860300768348\n",
            "Total Timesteps: 264424 Episode Num: 297 Reward: 820.0571023962243\n",
            "Total Timesteps: 265424 Episode Num: 298 Reward: 675.7117030868417\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 696.714071\n",
            "---------------------------------------\n",
            "Total Timesteps: 266424 Episode Num: 299 Reward: 637.0310099267784\n",
            "Total Timesteps: 267424 Episode Num: 300 Reward: 498.5390714440778\n",
            "Total Timesteps: 268424 Episode Num: 301 Reward: 681.1786810752589\n",
            "Total Timesteps: 269424 Episode Num: 302 Reward: 838.2045155071205\n",
            "Total Timesteps: 270424 Episode Num: 303 Reward: 921.0151018551966\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 796.633150\n",
            "---------------------------------------\n",
            "Total Timesteps: 271424 Episode Num: 304 Reward: 890.9577127355632\n",
            "Total Timesteps: 272424 Episode Num: 305 Reward: 978.3949620143687\n",
            "Total Timesteps: 273424 Episode Num: 306 Reward: 1010.7098045919311\n",
            "Total Timesteps: 274424 Episode Num: 307 Reward: 858.5029019182343\n",
            "Total Timesteps: 275424 Episode Num: 308 Reward: 737.7086713970859\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 733.882078\n",
            "---------------------------------------\n",
            "Total Timesteps: 276424 Episode Num: 309 Reward: 583.6639928789874\n",
            "Total Timesteps: 277424 Episode Num: 310 Reward: 581.6338552086127\n",
            "Total Timesteps: 278424 Episode Num: 311 Reward: 764.8324866018148\n",
            "Total Timesteps: 279424 Episode Num: 312 Reward: 873.3651206963666\n",
            "Total Timesteps: 280424 Episode Num: 313 Reward: 751.4497615968606\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 815.782561\n",
            "---------------------------------------\n",
            "Total Timesteps: 281424 Episode Num: 314 Reward: 594.0310375295095\n",
            "Total Timesteps: 282424 Episode Num: 315 Reward: 854.970325348138\n",
            "Total Timesteps: 283424 Episode Num: 316 Reward: 961.5871563077438\n",
            "Total Timesteps: 284424 Episode Num: 317 Reward: 779.7153806252888\n",
            "Total Timesteps: 285424 Episode Num: 318 Reward: 878.7152984247764\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 814.520815\n",
            "---------------------------------------\n",
            "Total Timesteps: 286424 Episode Num: 319 Reward: 927.4229218614394\n",
            "Total Timesteps: 287424 Episode Num: 320 Reward: 996.1044252891401\n",
            "Total Timesteps: 288424 Episode Num: 321 Reward: 868.37313758438\n",
            "Total Timesteps: 289424 Episode Num: 322 Reward: 866.7606340786948\n",
            "Total Timesteps: 290424 Episode Num: 323 Reward: 977.320534980959\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 818.998766\n",
            "---------------------------------------\n",
            "Total Timesteps: 291424 Episode Num: 324 Reward: 864.1176255421747\n",
            "Total Timesteps: 292424 Episode Num: 325 Reward: 727.8784325749375\n",
            "Total Timesteps: 293424 Episode Num: 326 Reward: 849.3987980160769\n",
            "Total Timesteps: 294424 Episode Num: 327 Reward: 910.857495930749\n",
            "Total Timesteps: 295424 Episode Num: 328 Reward: 879.3409109799865\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 759.332178\n",
            "---------------------------------------\n",
            "Total Timesteps: 296424 Episode Num: 329 Reward: 428.01587501473495\n",
            "Total Timesteps: 297424 Episode Num: 330 Reward: 910.5594345980878\n",
            "Total Timesteps: 298424 Episode Num: 331 Reward: 815.612202526098\n",
            "Total Timesteps: 299424 Episode Num: 332 Reward: 870.691705273507\n",
            "Total Timesteps: 300424 Episode Num: 333 Reward: 904.898095297627\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 859.233308\n",
            "---------------------------------------\n",
            "Total Timesteps: 301424 Episode Num: 334 Reward: 853.9216009855892\n",
            "Total Timesteps: 302424 Episode Num: 335 Reward: 861.5478948391661\n",
            "Total Timesteps: 303424 Episode Num: 336 Reward: 975.3683440908956\n",
            "Total Timesteps: 304424 Episode Num: 337 Reward: 903.2042194261361\n",
            "Total Timesteps: 305424 Episode Num: 338 Reward: 1019.5949109326641\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 884.053749\n",
            "---------------------------------------\n",
            "Total Timesteps: 306424 Episode Num: 339 Reward: 836.1292012429743\n",
            "Total Timesteps: 307424 Episode Num: 340 Reward: 1035.5811745909573\n",
            "Total Timesteps: 308424 Episode Num: 341 Reward: 974.3481065749885\n",
            "Total Timesteps: 309424 Episode Num: 342 Reward: 1129.2990497614283\n",
            "Total Timesteps: 310424 Episode Num: 343 Reward: 932.8941084714472\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 866.448900\n",
            "---------------------------------------\n",
            "Total Timesteps: 311424 Episode Num: 344 Reward: 791.151908456871\n",
            "Total Timesteps: 312424 Episode Num: 345 Reward: 757.1930628731474\n",
            "Total Timesteps: 313424 Episode Num: 346 Reward: 952.7114078118494\n",
            "Total Timesteps: 314424 Episode Num: 347 Reward: 794.5958196959393\n",
            "Total Timesteps: 315424 Episode Num: 348 Reward: 546.5539604480803\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 856.037906\n",
            "---------------------------------------\n",
            "Total Timesteps: 316424 Episode Num: 349 Reward: 998.4866524020902\n",
            "Total Timesteps: 317424 Episode Num: 350 Reward: 997.1894260069932\n",
            "Total Timesteps: 318424 Episode Num: 351 Reward: 825.0276392695698\n",
            "Total Timesteps: 319424 Episode Num: 352 Reward: 1081.4273966372857\n",
            "Total Timesteps: 320424 Episode Num: 353 Reward: 824.7263399065963\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1019.820029\n",
            "---------------------------------------\n",
            "Total Timesteps: 321424 Episode Num: 354 Reward: 1049.1239357005338\n",
            "Total Timesteps: 322424 Episode Num: 355 Reward: 1011.1391290184882\n",
            "Total Timesteps: 323424 Episode Num: 356 Reward: 499.81449221810504\n",
            "Total Timesteps: 324424 Episode Num: 357 Reward: 1016.3994420704191\n",
            "Total Timesteps: 325424 Episode Num: 358 Reward: 1056.2823910102888\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1012.398719\n",
            "---------------------------------------\n",
            "Total Timesteps: 326424 Episode Num: 359 Reward: 464.5962952709489\n",
            "Total Timesteps: 327424 Episode Num: 360 Reward: 889.7605587691676\n",
            "Total Timesteps: 328424 Episode Num: 361 Reward: 940.0052486066152\n",
            "Total Timesteps: 329424 Episode Num: 362 Reward: 1201.045698463105\n",
            "Total Timesteps: 330424 Episode Num: 363 Reward: 963.1602896119094\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 825.339152\n",
            "---------------------------------------\n",
            "Total Timesteps: 331424 Episode Num: 364 Reward: 692.9229620772398\n",
            "Total Timesteps: 332424 Episode Num: 365 Reward: 639.1637764928563\n",
            "Total Timesteps: 333424 Episode Num: 366 Reward: 1144.3484534313593\n",
            "Total Timesteps: 334424 Episode Num: 367 Reward: 967.153633583333\n",
            "Total Timesteps: 335424 Episode Num: 368 Reward: 1144.743699090152\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1017.324625\n",
            "---------------------------------------\n",
            "Total Timesteps: 336424 Episode Num: 369 Reward: 1022.6309331842134\n",
            "Total Timesteps: 337424 Episode Num: 370 Reward: 1072.6238995625376\n",
            "Total Timesteps: 338424 Episode Num: 371 Reward: 1010.3213541518479\n",
            "Total Timesteps: 339424 Episode Num: 372 Reward: 957.1356198530045\n",
            "Total Timesteps: 340424 Episode Num: 373 Reward: 862.5912141163807\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 976.883317\n",
            "---------------------------------------\n",
            "Total Timesteps: 341424 Episode Num: 374 Reward: 611.6684561119299\n",
            "Total Timesteps: 342424 Episode Num: 375 Reward: 926.4730734027229\n",
            "Total Timesteps: 343424 Episode Num: 376 Reward: 1248.8199886736136\n",
            "Total Timesteps: 344424 Episode Num: 377 Reward: 1048.5861050948786\n",
            "Total Timesteps: 345424 Episode Num: 378 Reward: 1307.6713305392684\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1211.094991\n",
            "---------------------------------------\n",
            "Total Timesteps: 346424 Episode Num: 379 Reward: 1274.936170844857\n",
            "Total Timesteps: 347424 Episode Num: 380 Reward: 1342.4941663312395\n",
            "Total Timesteps: 348424 Episode Num: 381 Reward: 874.3595652952098\n",
            "Total Timesteps: 349424 Episode Num: 382 Reward: 1155.114064409653\n",
            "Total Timesteps: 350424 Episode Num: 383 Reward: 831.8219515775154\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1225.281411\n",
            "---------------------------------------\n",
            "Total Timesteps: 351424 Episode Num: 384 Reward: 1309.5092574911014\n",
            "Total Timesteps: 352424 Episode Num: 385 Reward: 1280.2959920006533\n",
            "Total Timesteps: 353424 Episode Num: 386 Reward: 1022.4193450684098\n",
            "Total Timesteps: 354424 Episode Num: 387 Reward: 1178.3333864563203\n",
            "Total Timesteps: 355424 Episode Num: 388 Reward: 1305.2243691549236\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1238.144950\n",
            "---------------------------------------\n",
            "Total Timesteps: 356424 Episode Num: 389 Reward: 1247.7099639949292\n",
            "Total Timesteps: 357424 Episode Num: 390 Reward: 1045.8850639346645\n",
            "Total Timesteps: 358424 Episode Num: 391 Reward: 1463.905458882181\n",
            "Total Timesteps: 359424 Episode Num: 392 Reward: 1461.3588556209768\n",
            "Total Timesteps: 360424 Episode Num: 393 Reward: 1068.678802491407\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1302.020194\n",
            "---------------------------------------\n",
            "Total Timesteps: 361424 Episode Num: 394 Reward: 1299.7137653855277\n",
            "Total Timesteps: 362424 Episode Num: 395 Reward: 1457.435592914007\n",
            "Total Timesteps: 363424 Episode Num: 396 Reward: 1236.5412289863857\n",
            "Total Timesteps: 364424 Episode Num: 397 Reward: 1479.1437394188295\n",
            "Total Timesteps: 365424 Episode Num: 398 Reward: 1288.239861317609\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1201.692829\n",
            "---------------------------------------\n",
            "Total Timesteps: 366424 Episode Num: 399 Reward: 1178.0471992736097\n",
            "Total Timesteps: 367424 Episode Num: 400 Reward: 1298.4960444215742\n",
            "Total Timesteps: 368424 Episode Num: 401 Reward: 1280.2404270662666\n",
            "Total Timesteps: 369424 Episode Num: 402 Reward: 1386.631926936306\n",
            "Total Timesteps: 370424 Episode Num: 403 Reward: 1356.0141722838393\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 899.472401\n",
            "---------------------------------------\n",
            "Total Timesteps: 371424 Episode Num: 404 Reward: 708.9518515946437\n",
            "Total Timesteps: 372424 Episode Num: 405 Reward: 1451.1014781328295\n",
            "Total Timesteps: 373424 Episode Num: 406 Reward: 1295.7163046530127\n",
            "Total Timesteps: 374424 Episode Num: 407 Reward: 1460.9190289962075\n",
            "Total Timesteps: 375424 Episode Num: 408 Reward: 1276.2159939067715\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1535.293142\n",
            "---------------------------------------\n",
            "Total Timesteps: 376424 Episode Num: 409 Reward: 1540.1233242066714\n",
            "Total Timesteps: 377424 Episode Num: 410 Reward: 1603.2581934863472\n",
            "Total Timesteps: 378424 Episode Num: 411 Reward: 1531.479752080643\n",
            "Total Timesteps: 379424 Episode Num: 412 Reward: 1420.3041015923739\n",
            "Total Timesteps: 380424 Episode Num: 413 Reward: 1623.5239687019794\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1587.459891\n",
            "---------------------------------------\n",
            "Total Timesteps: 381424 Episode Num: 414 Reward: 1636.7598595351176\n",
            "Total Timesteps: 382424 Episode Num: 415 Reward: 1634.6584656142377\n",
            "Total Timesteps: 383424 Episode Num: 416 Reward: 1485.7513170173997\n",
            "Total Timesteps: 384424 Episode Num: 417 Reward: 1498.7863045787067\n",
            "Total Timesteps: 385424 Episode Num: 418 Reward: 1209.0096496879742\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1556.717281\n",
            "---------------------------------------\n",
            "Total Timesteps: 386424 Episode Num: 419 Reward: 1525.962165253322\n",
            "Total Timesteps: 387424 Episode Num: 420 Reward: 1820.3134957037826\n",
            "Total Timesteps: 388424 Episode Num: 421 Reward: 1717.5758918937495\n",
            "Total Timesteps: 389424 Episode Num: 422 Reward: 1659.09973005061\n",
            "Total Timesteps: 390424 Episode Num: 423 Reward: 1592.5493491467607\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1476.018197\n",
            "---------------------------------------\n",
            "Total Timesteps: 391424 Episode Num: 424 Reward: 1244.1276186244634\n",
            "Total Timesteps: 392424 Episode Num: 425 Reward: 1641.0786410971978\n",
            "Total Timesteps: 393424 Episode Num: 426 Reward: 1583.1379584927981\n",
            "Total Timesteps: 394424 Episode Num: 427 Reward: 1692.1824404883534\n",
            "Total Timesteps: 395424 Episode Num: 428 Reward: 1994.243003481792\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1662.166301\n",
            "---------------------------------------\n",
            "Total Timesteps: 396424 Episode Num: 429 Reward: 1641.661886007203\n",
            "Total Timesteps: 397424 Episode Num: 430 Reward: 1631.025583846356\n",
            "Total Timesteps: 398424 Episode Num: 431 Reward: 1434.5789834151408\n",
            "Total Timesteps: 399424 Episode Num: 432 Reward: 1776.7703627709836\n",
            "Total Timesteps: 400424 Episode Num: 433 Reward: 1658.2680881573224\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1821.721920\n",
            "---------------------------------------\n",
            "Total Timesteps: 401424 Episode Num: 434 Reward: 1767.807053256368\n",
            "Total Timesteps: 402424 Episode Num: 435 Reward: 1576.1546125080765\n",
            "Total Timesteps: 403424 Episode Num: 436 Reward: 1895.5790219698185\n",
            "Total Timesteps: 404424 Episode Num: 437 Reward: 1772.3014929154792\n",
            "Total Timesteps: 405424 Episode Num: 438 Reward: 1748.2226711594917\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1643.728639\n",
            "---------------------------------------\n",
            "Total Timesteps: 406424 Episode Num: 439 Reward: 1620.8712677481521\n",
            "Total Timesteps: 407424 Episode Num: 440 Reward: 1840.5746687081626\n",
            "Total Timesteps: 408424 Episode Num: 441 Reward: 1674.526911299464\n",
            "Total Timesteps: 409424 Episode Num: 442 Reward: 1730.115441081493\n",
            "Total Timesteps: 410424 Episode Num: 443 Reward: 1746.5557196875038\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1711.296453\n",
            "---------------------------------------\n",
            "Total Timesteps: 411424 Episode Num: 444 Reward: 1646.3418693613703\n",
            "Total Timesteps: 412424 Episode Num: 445 Reward: 1840.8795060885814\n",
            "Total Timesteps: 413424 Episode Num: 446 Reward: 1835.112750297392\n",
            "Total Timesteps: 414424 Episode Num: 447 Reward: 1648.6264335844503\n",
            "Total Timesteps: 415424 Episode Num: 448 Reward: 1493.978408961348\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 911.885863\n",
            "---------------------------------------\n",
            "Total Timesteps: 416424 Episode Num: 449 Reward: 925.0298269907498\n",
            "Total Timesteps: 417424 Episode Num: 450 Reward: 1645.6381540997947\n",
            "Total Timesteps: 418424 Episode Num: 451 Reward: 1779.2796613056548\n",
            "Total Timesteps: 419424 Episode Num: 452 Reward: 1935.3130727512455\n",
            "Total Timesteps: 420424 Episode Num: 453 Reward: 2010.4689122222019\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2023.981079\n",
            "---------------------------------------\n",
            "Total Timesteps: 421424 Episode Num: 454 Reward: 2000.914475265579\n",
            "Total Timesteps: 422424 Episode Num: 455 Reward: 1959.8174501243282\n",
            "Total Timesteps: 423424 Episode Num: 456 Reward: 1957.0127680672038\n",
            "Total Timesteps: 424424 Episode Num: 457 Reward: 1841.8713653137083\n",
            "Total Timesteps: 425424 Episode Num: 458 Reward: 1986.2020039916445\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1858.556016\n",
            "---------------------------------------\n",
            "Total Timesteps: 426424 Episode Num: 459 Reward: 1818.078945941302\n",
            "Total Timesteps: 427424 Episode Num: 460 Reward: 1893.49834103782\n",
            "Total Timesteps: 428424 Episode Num: 461 Reward: 1792.9204308564954\n",
            "Total Timesteps: 429424 Episode Num: 462 Reward: 1792.8266502044758\n",
            "Total Timesteps: 430424 Episode Num: 463 Reward: 1790.8330186263313\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1921.454592\n",
            "---------------------------------------\n",
            "Total Timesteps: 431424 Episode Num: 464 Reward: 1769.019415691689\n",
            "Total Timesteps: 432424 Episode Num: 465 Reward: 1830.3012030311586\n",
            "Total Timesteps: 433424 Episode Num: 466 Reward: 1770.9504889530042\n",
            "Total Timesteps: 434424 Episode Num: 467 Reward: 1989.4116626826417\n",
            "Total Timesteps: 435424 Episode Num: 468 Reward: 1741.9363391225995\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1956.833841\n",
            "---------------------------------------\n",
            "Total Timesteps: 436424 Episode Num: 469 Reward: 1959.5221073810494\n",
            "Total Timesteps: 437424 Episode Num: 470 Reward: 1822.3632915603007\n",
            "Total Timesteps: 438424 Episode Num: 471 Reward: 1915.3273799702142\n",
            "Total Timesteps: 439424 Episode Num: 472 Reward: 1783.141742554605\n",
            "Total Timesteps: 440424 Episode Num: 473 Reward: 1569.918836990425\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1840.495188\n",
            "---------------------------------------\n",
            "Total Timesteps: 441424 Episode Num: 474 Reward: 1822.897707006448\n",
            "Total Timesteps: 442424 Episode Num: 475 Reward: 1889.8579850087008\n",
            "Total Timesteps: 443424 Episode Num: 476 Reward: 1800.034141948675\n",
            "Total Timesteps: 444424 Episode Num: 477 Reward: 1892.0036459477012\n",
            "Total Timesteps: 445424 Episode Num: 478 Reward: 2147.281695019533\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1988.194171\n",
            "---------------------------------------\n",
            "Total Timesteps: 446424 Episode Num: 479 Reward: 1926.9845338410516\n",
            "Total Timesteps: 447424 Episode Num: 480 Reward: 2041.1294056842594\n",
            "Total Timesteps: 448424 Episode Num: 481 Reward: 1649.6597612157561\n",
            "Total Timesteps: 449424 Episode Num: 482 Reward: 1885.4549078644327\n",
            "Total Timesteps: 450424 Episode Num: 483 Reward: 2021.915005588512\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1924.406500\n",
            "---------------------------------------\n",
            "Total Timesteps: 451424 Episode Num: 484 Reward: 1981.734863382624\n",
            "Total Timesteps: 452424 Episode Num: 485 Reward: 1944.2647873204915\n",
            "Total Timesteps: 453424 Episode Num: 486 Reward: 2165.6692024935132\n",
            "Total Timesteps: 454424 Episode Num: 487 Reward: 1909.8371790258186\n",
            "Total Timesteps: 455424 Episode Num: 488 Reward: 1895.2891816646468\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1945.748904\n",
            "---------------------------------------\n",
            "Total Timesteps: 456424 Episode Num: 489 Reward: 2166.878211071712\n",
            "Total Timesteps: 457424 Episode Num: 490 Reward: 1827.746129947309\n",
            "Total Timesteps: 458424 Episode Num: 491 Reward: 1789.2946629504006\n",
            "Total Timesteps: 459424 Episode Num: 492 Reward: 2047.145352813419\n",
            "Total Timesteps: 460424 Episode Num: 493 Reward: 1240.3630199324227\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1580.249623\n",
            "---------------------------------------\n",
            "Total Timesteps: 461424 Episode Num: 494 Reward: 1514.6666773632514\n",
            "Total Timesteps: 462424 Episode Num: 495 Reward: 2046.8483218854226\n",
            "Total Timesteps: 463424 Episode Num: 496 Reward: 2162.1518143943454\n",
            "Total Timesteps: 464424 Episode Num: 497 Reward: 2114.3417392106767\n",
            "Total Timesteps: 465424 Episode Num: 498 Reward: 1858.217963423974\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2187.265464\n",
            "---------------------------------------\n",
            "Total Timesteps: 466424 Episode Num: 499 Reward: 2202.578572590004\n",
            "Total Timesteps: 467424 Episode Num: 500 Reward: 1859.5767827822192\n",
            "Total Timesteps: 468424 Episode Num: 501 Reward: 1863.2961362145707\n",
            "Total Timesteps: 469424 Episode Num: 502 Reward: 1779.1591850243487\n",
            "Total Timesteps: 470424 Episode Num: 503 Reward: 1870.0677190108538\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2199.518383\n",
            "---------------------------------------\n",
            "Total Timesteps: 471424 Episode Num: 504 Reward: 2191.966344938526\n",
            "Total Timesteps: 472424 Episode Num: 505 Reward: 1933.3627413025883\n",
            "Total Timesteps: 473424 Episode Num: 506 Reward: 2063.18926550323\n",
            "Total Timesteps: 474424 Episode Num: 507 Reward: 2176.065537167481\n",
            "Total Timesteps: 475424 Episode Num: 508 Reward: 2199.974111462143\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2189.797563\n",
            "---------------------------------------\n",
            "Total Timesteps: 476424 Episode Num: 509 Reward: 2212.3995307238038\n",
            "Total Timesteps: 477424 Episode Num: 510 Reward: 1747.8398867318872\n",
            "Total Timesteps: 478424 Episode Num: 511 Reward: 2260.6880878093802\n",
            "Total Timesteps: 479424 Episode Num: 512 Reward: 1969.1926835593865\n",
            "Total Timesteps: 480424 Episode Num: 513 Reward: 2116.1390365243255\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2210.134658\n",
            "---------------------------------------\n",
            "Total Timesteps: 481424 Episode Num: 514 Reward: 2230.1225463624914\n",
            "Total Timesteps: 482424 Episode Num: 515 Reward: 2167.8907352982496\n",
            "Total Timesteps: 483424 Episode Num: 516 Reward: 2299.252570766115\n",
            "Total Timesteps: 484424 Episode Num: 517 Reward: 2301.858266357208\n",
            "Total Timesteps: 485424 Episode Num: 518 Reward: 2222.947012135624\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1712.678480\n",
            "---------------------------------------\n",
            "Total Timesteps: 486424 Episode Num: 519 Reward: 1740.9792050575093\n",
            "Total Timesteps: 487424 Episode Num: 520 Reward: 2316.1116429630915\n",
            "Total Timesteps: 488424 Episode Num: 521 Reward: 2292.003840267436\n",
            "Total Timesteps: 489424 Episode Num: 522 Reward: 2281.33320110964\n",
            "Total Timesteps: 490424 Episode Num: 523 Reward: 1969.3879251020412\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2070.046342\n",
            "---------------------------------------\n",
            "Total Timesteps: 491424 Episode Num: 524 Reward: 2082.5895205438437\n",
            "Total Timesteps: 492424 Episode Num: 525 Reward: 2296.0962762741833\n",
            "Total Timesteps: 493424 Episode Num: 526 Reward: 2261.489319699288\n",
            "Total Timesteps: 494424 Episode Num: 527 Reward: 2216.012745429353\n",
            "Total Timesteps: 495424 Episode Num: 528 Reward: 2263.5425992176765\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2212.158372\n",
            "---------------------------------------\n",
            "Total Timesteps: 496424 Episode Num: 529 Reward: 2257.7576415783856\n",
            "Total Timesteps: 497424 Episode Num: 530 Reward: 2254.8887545480543\n",
            "Total Timesteps: 498424 Episode Num: 531 Reward: 2128.0102542784807\n",
            "Total Timesteps: 499424 Episode Num: 532 Reward: 2241.4846277783563\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2137.727300\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi6e2-_pu05e",
        "colab_type": "text"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oW4d1YAMqif1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "outputId": "9a938762-34d8-4a00-ba5f-7f7cd89506e9"
      },
      "source": [
        "class Actor(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    super(Actor, self).__init__()\n",
        "    self.layer_1 = nn.Linear(state_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, action_dim)\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.layer_1(x))\n",
        "    x = F.relu(self.layer_2(x))\n",
        "    x = self.max_action * torch.tanh(self.layer_3(x)) \n",
        "    return x\n",
        "\n",
        "class Critic(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    super(Critic, self).__init__()\n",
        "    # Defining the first Critic neural network\n",
        "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, 1)\n",
        "    # Defining the second Critic neural network\n",
        "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_5 = nn.Linear(400, 300)\n",
        "    self.layer_6 = nn.Linear(300, 1)\n",
        "\n",
        "  def forward(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    # Forward-Propagation on the first Critic Neural Network\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    # Forward-Propagation on the second Critic Neural Network\n",
        "    x2 = F.relu(self.layer_4(xu))\n",
        "    x2 = F.relu(self.layer_5(x2))\n",
        "    x2 = self.layer_6(x2)\n",
        "    return x1, x2\n",
        "\n",
        "  def Q1(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    return x1\n",
        "\n",
        "# Selecting the device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Building the whole Training Process into a class\n",
        "\n",
        "class TD3(object):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "    self.critic = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def select_action(self, state):\n",
        "    state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
        "    return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "    \n",
        "    for it in range(iterations):\n",
        "      \n",
        "      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
        "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "      state = torch.Tensor(batch_states).to(device)\n",
        "      next_state = torch.Tensor(batch_next_states).to(device)\n",
        "      action = torch.Tensor(batch_actions).to(device)\n",
        "      reward = torch.Tensor(batch_rewards).to(device)\n",
        "      done = torch.Tensor(batch_dones).to(device)\n",
        "      \n",
        "      # Step 5: From the next state s’, the Actor target plays the next action a’\n",
        "      next_action = self.actor_target(next_state)\n",
        "      \n",
        "      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
        "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
        "      noise = noise.clamp(-noise_clip, noise_clip)\n",
        "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
        "      \n",
        "      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
        "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "      \n",
        "      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
        "      target_Q = torch.min(target_Q1, target_Q2)\n",
        "      \n",
        "      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
        "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
        "      \n",
        "      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
        "      current_Q1, current_Q2 = self.critic(state, action)\n",
        "      \n",
        "      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
        "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "      \n",
        "      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
        "      self.critic_optimizer.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      self.critic_optimizer.step()\n",
        "      \n",
        "      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
        "      if it % policy_freq == 0:\n",
        "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "        \n",
        "        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "        \n",
        "        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "  \n",
        "  # Making a save method to save a trained model\n",
        "  def save(self, filename, directory):\n",
        "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        "  \n",
        "  # Making a load method to load a pre-trained model\n",
        "  def load(self, filename, directory):\n",
        "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))\n",
        "\n",
        "def evaluate_policy(policy, eval_episodes=10):\n",
        "  avg_reward = 0.\n",
        "  for _ in range(eval_episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = policy.select_action(np.array(obs))\n",
        "      obs, reward, done, _ = env.step(action)\n",
        "      avg_reward += reward\n",
        "  avg_reward /= eval_episodes\n",
        "  print (\"---------------------------------------\")\n",
        "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
        "  print (\"---------------------------------------\")\n",
        "  return avg_reward\n",
        "\n",
        "env_name = \"AntBulletEnv-v0\"\n",
        "seed = 0\n",
        "\n",
        "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Settings: %s\" % (file_name))\n",
        "print (\"---------------------------------------\")\n",
        "\n",
        "eval_episodes = 10\n",
        "save_env_vid = True\n",
        "env = gym.make(env_name)\n",
        "max_episode_steps = env._max_episode_steps\n",
        "if save_env_vid:\n",
        "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
        "  env.reset()\n",
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])\n",
        "policy = TD3(state_dim, action_dim, max_action)\n",
        "policy.load(file_name, './pytorch_models/')\n",
        "_ = evaluate_policy(policy, eval_episodes=eval_episodes)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Settings: TD3_AntBulletEnv-v0_0\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2136.279812\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcnexWrW4a8P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}